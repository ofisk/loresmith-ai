// Sync Queue Service for managing file indexing operations

import { FileDAO } from "@/dao/file-dao";
import { LibraryRAGService } from "@/services/rag/rag-service";
import type { FileMetadata } from "@/types/upload";
import {
  notifyMetadataAutoGenerated,
  notifyFileIndexingStatus,
} from "@/lib/notifications";
import type { FileProcessingResult } from "@/types/file-processing";
import { ChunkedProcessingService } from "./chunked-processing-service";
import { MemoryLimitError } from "@/lib/errors";
import { getUniqueDisplayName } from "@/lib/file-utils";

export class SyncQueueService {
  /**
   * Process a file upload - index file directly with LibraryRAGService
   */
  static async processFileUpload(
    env: any,
    username: string,
    fileKey: string,
    fileName: string,
    _jwt?: string
  ): Promise<FileProcessingResult> {
    const startTime = Date.now();
    console.log(`[DEBUG] [SyncQueue] ===== PROCESSING FILE UPLOAD =====`);
    console.log(`[DEBUG] [SyncQueue] File: ${fileName}`);
    console.log(`[DEBUG] [SyncQueue] File Key: ${fileKey}`);
    console.log(`[DEBUG] [SyncQueue] User: ${username}`);
    console.log(`[DEBUG] [SyncQueue] Timestamp: ${new Date().toISOString()}`);

    const fileDAO = new FileDAO(env.DB);

    try {
      // Get file from R2
      const file = await env.R2.get(fileKey);
      if (!file) {
        throw new Error(`File not found: ${fileKey}`);
      }

      // Get file metadata from database
      const dbMetadata = await fileDAO.getFileForRag(fileKey, username);
      if (!dbMetadata) {
        throw new Error(`File metadata not found in database: ${fileKey}`);
      }

      const ragService = new LibraryRAGService(env);
      const contentType =
        dbMetadata.content_type || file.httpMetadata?.contentType || "";

      // Check if file should be queued for background processing
      // Large files (>100MB) should be queued to avoid timeout during processing
      const queueCheck = await ragService.shouldQueueFile(file, contentType);
      if (queueCheck.shouldQueue) {
        console.log(
          `[DEBUG] [SyncQueue] File ${fileName} should be queued: ${queueCheck.reason}`
        );

        // Add file to sync queue for background processing
        await fileDAO.addToSyncQueue(
          username,
          fileKey,
          fileName,
          fileKey // Use fileKey as rag_id (required field)
        );

        // Update file status to SYNCING to indicate it's being processed
        await fileDAO.updateFileRecord(fileKey, FileDAO.STATUS.SYNCING);

        console.log(
          `[DEBUG] [SyncQueue] File ${fileName} added to sync queue for background processing`
        );

        // Trigger queue processing in the background (non-blocking)
        // This ensures the file starts processing immediately
        // Note: Periodic queue processing (via scheduled cron) will also retry if this fails
        SyncQueueService.processSyncQueue(env, username, _jwt).catch(
          (error: unknown) => {
            console.error(
              `[SyncQueue] Failed to trigger queue processing for ${fileName}:`,
              error
            );
            // Don't mark file as ERROR here - periodic processing will retry
            // If periodic processing also fails, cleanup will handle it
          }
        );

        return {
          success: true,
          queued: true,
          message: `File ${fileName} has been queued for background processing due to its size. It will be processed shortly.`,
        };
      }

      // Process file directly with LibraryRAGService
      console.log(
        `[DEBUG] [SyncQueue] Processing file with LibraryRAGService...`
      );
      const fileMetadata: FileMetadata = {
        id: dbMetadata.file_key, // Use file_key as id since processFile expects id
        fileKey: dbMetadata.file_key,
        userId: dbMetadata.username,
        filename: dbMetadata.file_name,
        fileSize: dbMetadata.file_size || 0,
        contentType:
          dbMetadata.content_type || file.httpMetadata?.contentType || "",
        tags: dbMetadata.tags ? JSON.parse(dbMetadata.tags) : [],
        status: dbMetadata.status || "uploaded",
        createdAt: dbMetadata.created_at || new Date().toISOString(),
        updatedAt: dbMetadata.updated_at || new Date().toISOString(),
      };

      const processResult = await ragService.processFile(fileMetadata);

      // Check if file was chunked (explicit flag)
      if (processResult.chunked) {
        console.log(
          `[SyncQueue] File ${fileName} was chunked. Chunks will be processed separately.`
        );
        // Return success - chunks will be processed in background
        return {
          success: true,
          queued: true,
          message: `File ${fileName} has been split into chunks for processing. Chunks will be processed in the background.`,
        };
      }

      // Check if file was actually indexed (has vectorId)
      if (!processResult.vectorId) {
        throw new Error(
          "File indexing failed: embeddings were not stored. The file may be too large or contain unprocessable content."
        );
      }

      // Save generated metadata, respecting user-provided values
      const metadataUpdates: {
        display_name?: string;
        description?: string;
        tags?: string;
      } = {};

      // Only update fields that are empty/null in database (user-provided values take precedence)
      if (processResult.displayName && !dbMetadata.display_name) {
        // Check for display name collisions and get a unique display name
        metadataUpdates.display_name = await getUniqueDisplayName(
          (username, displayName, excludeFileKey) =>
            fileDAO.displayNameExistsForUser(
              username,
              displayName,
              excludeFileKey
            ),
          processResult.displayName,
          username,
          fileKey
        );
      }
      if (processResult.description && !dbMetadata.description) {
        metadataUpdates.description = processResult.description;
      }
      if (processResult.tags && processResult.tags.length > 0) {
        const existingTags = dbMetadata.tags ? JSON.parse(dbMetadata.tags) : [];
        if (existingTags.length === 0) {
          metadataUpdates.tags = JSON.stringify(processResult.tags);
        }
      }

      // Save metadata updates if any
      if (Object.keys(metadataUpdates).length > 0) {
        await fileDAO.updateFileMetadata(fileKey, metadataUpdates);

        // Send notification for auto-generated metadata
        try {
          await notifyMetadataAutoGenerated(env, username, fileName, {
            displayName: metadataUpdates.display_name,
            description: metadataUpdates.description,
            tags: metadataUpdates.tags
              ? JSON.parse(metadataUpdates.tags)
              : undefined,
          });
        } catch (notifError) {
          console.error(
            "[SyncQueue] Failed to send metadata generation notification:",
            notifError
          );
        }
      }

      // Update file status to completed (fully indexed and searchable)
      await fileDAO.updateFileRecord(fileKey, FileDAO.STATUS.COMPLETED);

      const endTime = Date.now();
      const duration = endTime - startTime;
      console.log(
        `[DEBUG] [SyncQueue] ===== FILE UPLOAD PROCESSING COMPLETED =====`
      );
      console.log(`[DEBUG] [SyncQueue] Duration: ${duration}ms`);
      console.log(`[DEBUG] [SyncQueue] Status: INDEXED`);

      return {
        success: true,
        queued: false,
        message: `File ${fileName} indexed successfully`,
      };
    } catch (error) {
      const endTime = Date.now();
      const duration = endTime - startTime;
      console.error(
        `[DEBUG] [SyncQueue] ===== FILE UPLOAD PROCESSING FAILED =====`
      );
      console.error(`[DEBUG] [SyncQueue] Duration: ${duration}ms`);
      console.error(
        `[DEBUG] [SyncQueue] Failed to process file ${fileName}:`,
        error
      );

      // Check if this is a memory limit error
      const isMemoryLimitError = MemoryLimitError.isMemoryLimitError(error);

      if (isMemoryLimitError) {
        // Store error code to prevent retries
        await fileDAO.updateFileRecordWithError(
          fileKey,
          FileDAO.STATUS.ERROR,
          error.errorCode,
          error.message
        );

        // Send user-friendly notification about memory limit
        try {
          await notifyFileIndexingStatus(
            env,
            username,
            fileKey,
            fileName,
            FileDAO.STATUS.ERROR,
            {
              visibility: "both",
              userMessage: `‚ö†Ô∏è "${fileName}" (${error.fileSizeMB.toFixed(2)}MB) exceeds our ${error.memoryLimitMB}MB limit. Please split the file into smaller parts or use a file under ${error.memoryLimitMB}MB.`,
              reason: error.errorCode,
              fileSize: error.fileSizeMB * 1024 * 1024,
            }
          );
        } catch (notifyError) {
          console.error(
            `[SyncQueue] Failed to send memory limit notification for ${fileName}:`,
            notifyError
          );
        }

        return {
          success: false,
          queued: false,
          message: `File ${fileName} is too large (${error.fileSizeMB.toFixed(2)}MB). Maximum size is ${error.memoryLimitMB}MB.`,
          error: error.errorCode,
        };
      }

      // For other errors, mark as error without error code (retryable)
      await fileDAO.updateFileRecord(fileKey, FileDAO.STATUS.ERROR);

      const errorMessage =
        error instanceof Error ? error.message : "Unknown error";
      return {
        success: false,
        queued: false,
        message: `File ${fileName} processing failed: ${errorMessage}`,
        error: errorMessage,
      };
    }
  }

  /**
   * Process the sync queue - process all queued files with LibraryRAGService
   */
  static async processSyncQueue(
    env: any,
    username: string,
    _jwt?: string
  ): Promise<{ processed: number; jobId?: string }> {
    const fileDAO = new FileDAO(env.DB);

    // Get all pending queue items for this user
    const queueItems = await fileDAO.getSyncQueue(username);

    if (queueItems.length === 0) {
      console.log(`[SyncQueue] No items in queue for user ${username}`);
      return { processed: 0 };
    }

    console.log(
      `[SyncQueue] Processing ${queueItems.length} queued items for user ${username}`
    );

    let processed = 0;
    const ragService = new LibraryRAGService(env);

    // Process each queued file
    for (const item of queueItems) {
      try {
        // Get file from R2
        const file = await env.R2.get(item.file_key);
        if (!file) {
          console.error(`[SyncQueue] File not found: ${item.file_key}`);
          await fileDAO.removeFromSyncQueue(item.file_key);
          continue;
        }

        // Get file metadata from database
        const dbMetadata = await fileDAO.getFileForRag(item.file_key, username);
        if (!dbMetadata) {
          console.error(
            `[SyncQueue] File metadata not found: ${item.file_key}`
          );
          await fileDAO.removeFromSyncQueue(item.file_key);
          continue;
        }

        // Check if file has processing chunks (chunked processing)
        const chunkedService = new ChunkedProcessingService(env);
        const hasChunks = await chunkedService.hasExistingChunks(item.file_key);

        if (hasChunks) {
          // Process file chunks
          await SyncQueueService.processFileChunks(
            env,
            item.file_key,
            username,
            item.file_name,
            file,
            dbMetadata
          );

          // Check if all chunks are complete
          const mergeResult = await chunkedService.mergeChunkResults(
            item.file_key
          );

          if (mergeResult.allComplete && mergeResult.allSuccessful) {
            // All chunks processed successfully - mark file as completed
            await fileDAO.updateFileRecord(
              item.file_key,
              FileDAO.STATUS.COMPLETED
            );
            await fileDAO.removeFromSyncQueue(item.file_key);
            processed++;
            console.log(
              `[SyncQueue] All chunks processed successfully for ${item.file_name}`
            );
          } else if (mergeResult.allComplete && !mergeResult.allSuccessful) {
            // All chunks processed but some failed - mark file as error
            console.error(
              `[SyncQueue] Some chunks failed for ${item.file_name}. Stats:`,
              mergeResult.stats
            );
            await fileDAO.updateFileRecord(item.file_key, FileDAO.STATUS.ERROR);
            await fileDAO.removeFromSyncQueue(item.file_key);
          } else {
            // Chunks still processing - keep in queue
            console.log(
              `[SyncQueue] File ${item.file_name} has chunks still processing. Stats:`,
              mergeResult.stats
            );
          }
          continue;
        }

        // Process file with LibraryRAGService (normal processing)
        const fileMetadata: FileMetadata = {
          id: dbMetadata.file_key,
          fileKey: dbMetadata.file_key,
          userId: dbMetadata.username,
          filename: dbMetadata.file_name,
          fileSize: dbMetadata.file_size || 0,
          contentType:
            dbMetadata.content_type || file.httpMetadata?.contentType || "",
          tags: dbMetadata.tags ? JSON.parse(dbMetadata.tags) : [],
          status: dbMetadata.status || "uploaded",
          createdAt: dbMetadata.created_at || new Date().toISOString(),
          updatedAt: dbMetadata.updated_at || new Date().toISOString(),
        };

        const processResult = await ragService.processFile(fileMetadata);

        // Check if file was chunked (explicit flag)
        if (processResult.chunked) {
          console.log(
            `[SyncQueue] File ${item.file_name} was chunked. Chunks will be processed separately.`
          );
          // Keep in queue for chunk processing
          continue;
        }

        // Check if file was actually indexed (has vectorId)
        if (!processResult.vectorId) {
          throw new Error(
            `File indexing failed: embeddings were not stored. The file may be too large or contain unprocessable content.`
          );
        }

        // Save generated metadata, respecting user-provided values
        const metadataUpdates: {
          display_name?: string;
          description?: string;
          tags?: string;
        } = {};

        // Only update fields that are empty/null in database (user-provided values take precedence)
        if (processResult.displayName && !dbMetadata.display_name) {
          metadataUpdates.display_name = processResult.displayName;
        }
        if (processResult.description && !dbMetadata.description) {
          metadataUpdates.description = processResult.description;
        }
        if (processResult.tags && processResult.tags.length > 0) {
          const existingTags = dbMetadata.tags
            ? JSON.parse(dbMetadata.tags)
            : [];
          if (existingTags.length === 0) {
            metadataUpdates.tags = JSON.stringify(processResult.tags);
          }
        }

        // Save metadata updates if any
        if (Object.keys(metadataUpdates).length > 0) {
          await fileDAO.updateFileMetadata(item.file_key, metadataUpdates);
        }

        // Update file status to COMPLETED (fully indexed and searchable)
        await fileDAO.updateFileRecord(item.file_key, FileDAO.STATUS.COMPLETED);
        await fileDAO.removeFromSyncQueue(item.file_key);

        processed++;
        console.log(`[SyncQueue] Processed queued file ${item.file_name}`);
      } catch (error) {
        const errorMessage =
          error instanceof Error ? error.message : "Unknown error";
        const errorStack = error instanceof Error ? error.stack : undefined;

        console.error(
          `[SyncQueue] Failed to process queued file ${item.file_name}:`,
          errorMessage
        );
        if (errorStack) {
          console.error(`[SyncQueue] Error stack:`, errorStack);
        }

        // Get current retry count
        const currentRetryCount = (item.retry_count || 0) + 1;
        const MAX_RETRIES = 3;

        // Update retry count
        await fileDAO.updateSyncQueueRetryCount(
          item.file_key,
          currentRetryCount
        );

        // If we've exceeded max retries, mark as ERROR and remove from queue
        if (currentRetryCount >= MAX_RETRIES) {
          console.error(
            `[SyncQueue] Max retries (${MAX_RETRIES}) exceeded for ${item.file_name}, marking as ERROR`
          );

          const fileSizeMB = (item.file_size || 0) / (1024 * 1024);

          // Check if this is a memory limit error or network error on large file
          const memoryLimitError = MemoryLimitError.fromRuntimeError(
            error,
            fileSizeMB,
            128,
            item.file_key,
            item.file_name
          );

          // Network errors on large files (>100MB) are likely memory-related
          const isNetworkErrorOnLargeFile =
            (errorMessage.includes("Network connection lost") ||
              errorMessage.includes("network") ||
              errorMessage.includes("connection")) &&
            fileSizeMB > 100;

          if (
            memoryLimitError ||
            errorMessage.includes("Memory limit") ||
            errorMessage.includes("too large") ||
            isNetworkErrorOnLargeFile
          ) {
            // Store error code to prevent retries
            await fileDAO.updateFileRecordWithError(
              item.file_key,
              FileDAO.STATUS.ERROR,
              "MEMORY_LIMIT_EXCEEDED",
              errorMessage
            );

            // Send user-friendly notification about memory limit
            try {
              const userMessage = isNetworkErrorOnLargeFile
                ? `‚ö†Ô∏è "${item.file_name}" (${fileSizeMB.toFixed(2)}MB) is too large to process. The file caused network connection issues during processing, likely due to memory constraints. Please split the file into smaller parts (under 100MB each).`
                : `‚ö†Ô∏è "${item.file_name}" (${fileSizeMB.toFixed(2)}MB) exceeds our 128MB limit. Please split the file into smaller parts or use a file under 128MB.`;

              await notifyFileIndexingStatus(
                env,
                username,
                item.file_key,
                item.file_name,
                FileDAO.STATUS.ERROR,
                {
                  visibility: "both",
                  userMessage,
                  reason: "MEMORY_LIMIT_EXCEEDED",
                  fileSize: item.file_size || 0,
                }
              );
            } catch (notifyError) {
              console.error(
                `[SyncQueue] Failed to send memory limit notification for ${item.file_name}:`,
                notifyError
              );
            }
          } else {
            // For other errors, mark as error without error code (retryable)
            await fileDAO.updateFileRecord(item.file_key, FileDAO.STATUS.ERROR);

            // Send error notification with better message for network errors
            try {
              const isNetworkError =
                errorMessage.includes("Network connection lost") ||
                errorMessage.includes("network") ||
                errorMessage.includes("connection");

              const userMessage = isNetworkError
                ? `üõë Network connection issue while indexing "${item.file_name}". This may be temporary - please try again later or contact support if the issue persists.`
                : `üõë Our quill slipped while indexing "${item.file_name}". Please try again later.`;

              await notifyFileIndexingStatus(
                env,
                username,
                item.file_key,
                item.file_name,
                FileDAO.STATUS.ERROR,
                {
                  visibility: "both",
                  userMessage,
                  reason: errorMessage,
                }
              );
            } catch (notifyError) {
              console.error(
                `[SyncQueue] Failed to send error notification for ${item.file_name}:`,
                notifyError
              );
            }
          }

          // Remove from queue after max retries
          await fileDAO.removeFromSyncQueue(item.file_key);
        } else {
          // Keep in queue for automatic retry
          console.log(
            `[SyncQueue] File ${item.file_name} will be retried automatically (attempt ${currentRetryCount}/${MAX_RETRIES}). Error: ${errorMessage}`
          );
          // Don't mark as ERROR yet - keep in queue for retry
          // Status will remain as SYNCING until successful or max retries
        }
      }
    }

    return { processed };
  }

  /**
   * Process file chunks for a file that has been split into chunks
   */
  private static async processFileChunks(
    env: any,
    fileKey: string,
    _username: string,
    fileName: string,
    file: any,
    dbMetadata: any
  ): Promise<void> {
    const fileDAO = new FileDAO(env.DB);
    const chunkedService = new ChunkedProcessingService(env);

    // Get pending chunks for this file
    const chunks = await fileDAO.getFileProcessingChunks(fileKey);
    const pendingChunks = chunks.filter((chunk) => chunk.status === "pending");

    if (pendingChunks.length === 0) {
      console.log(`[SyncQueue] No pending chunks for file ${fileKey}`);
      return;
    }

    console.log(
      `[SyncQueue] Processing ${pendingChunks.length} pending chunk(s) for file ${fileName}`
    );

    const contentType =
      dbMetadata.content_type || file.httpMetadata?.contentType || "";
    const metadataId = dbMetadata.file_key;

    // Determine if we should load the full buffer based on file size
    // If file is chunked, it's too large to load in memory - skip trying
    const fileSizeMB = (dbMetadata.file_size || 0) / (1024 * 1024);
    const MEMORY_LIMIT_MB = 128;
    const SAFE_THRESHOLD_MB = 100; // For PDFs, be conservative

    // Check if file size indicates we should skip loading full buffer
    const shouldSkipFullBuffer =
      fileSizeMB > MEMORY_LIMIT_MB ||
      (contentType.includes("pdf") && fileSizeMB > SAFE_THRESHOLD_MB);

    let fileBuffer: ArrayBuffer | null = null;
    let usePerChunkFetch = shouldSkipFullBuffer;

    if (!shouldSkipFullBuffer) {
      // Only try to load full buffer if file size is safe
      try {
        fileBuffer = await file.arrayBuffer();
      } catch (bufferError) {
        console.warn(
          `[SyncQueue] Failed to load full file buffer for chunks ${fileKey}, will fetch per chunk:`,
          bufferError instanceof Error
            ? bufferError.message
            : String(bufferError)
        );
        usePerChunkFetch = true;
      }
    } else {
      console.log(
        `[SyncQueue] Skipping full buffer load for ${fileKey} (${fileSizeMB.toFixed(2)}MB) - file is too large, will fetch per chunk`
      );
    }

    // Process each pending chunk
    for (const chunk of pendingChunks) {
      try {
        // If we couldn't load the full buffer, fetch the file fresh for this chunk
        let chunkBuffer: ArrayBuffer;
        if (usePerChunkFetch) {
          const chunkFile = await env.R2.get(fileKey);
          if (!chunkFile) {
            throw new Error("File not found in R2");
          }
          try {
            chunkBuffer = await chunkFile.arrayBuffer();
          } catch (chunkBufferError) {
            // Even per-chunk fetch failed - file is too large for Worker memory
            throw new Error(
              `File too large to process: ${chunkBufferError instanceof Error ? chunkBufferError.message : "Memory limit exceeded"}`
            );
          }
        } else {
          chunkBuffer = fileBuffer!;
        }

        const chunkDefinition = {
          chunkIndex: chunk.chunkIndex,
          totalChunks: chunk.totalChunks,
          pageRangeStart: chunk.pageRangeStart,
          pageRangeEnd: chunk.pageRangeEnd,
          byteRangeStart: chunk.byteRangeStart,
          byteRangeEnd: chunk.byteRangeEnd,
        };

        await chunkedService.processChunk(
          chunk.id,
          fileKey,
          chunkDefinition,
          chunkBuffer,
          contentType,
          metadataId
        );

        console.log(
          `[SyncQueue] Successfully processed chunk ${chunk.chunkIndex + 1}/${chunk.totalChunks} for file ${fileName}`
        );
      } catch (chunkError) {
        const errorMessage =
          chunkError instanceof Error ? chunkError.message : String(chunkError);
        console.error(
          `[SyncQueue] Failed to process chunk ${chunk.chunkIndex + 1}/${chunk.totalChunks} for file ${fileName}:`,
          errorMessage
        );

        // Update chunk retry count
        const currentRetryCount = chunk.retryCount;
        const MAX_RETRIES = 3;

        if (currentRetryCount < MAX_RETRIES) {
          // Increment retry count and keep chunk as pending for next retry
          await fileDAO.updateFileProcessingChunk(chunk.id, {
            retryCount: currentRetryCount + 1,
            status: "pending", // Reset to pending for retry
          });
          console.log(
            `[SyncQueue] Chunk ${chunk.chunkIndex + 1} will be retried (attempt ${currentRetryCount + 1}/${MAX_RETRIES})`
          );
        } else {
          // Max retries exceeded - mark chunk as failed
          await fileDAO.updateFileProcessingChunk(chunk.id, {
            status: "failed",
            errorMessage: errorMessage,
            retryCount: currentRetryCount + 1,
          });
          console.error(
            `[SyncQueue] Chunk ${chunk.chunkIndex + 1} failed after ${MAX_RETRIES} retries`
          );
        }
      }
    }
  }

  /**
   * Check if a user has any queued items
   */
  static async hasQueuedItems(env: any, username: string): Promise<boolean> {
    const fileDAO = new FileDAO(env.DB);
    const queueItems = await fileDAO.getSyncQueue(username);
    return queueItems.length > 0;
  }

  /**
   * Get queue status for a user
   */
  static async getQueueStatus(
    env: any,
    username: string
  ): Promise<{
    queuedCount: number;
    ongoingJobs: boolean;
    queueItems: any[];
  }> {
    const fileDAO = new FileDAO(env.DB);

    const queueItems = await fileDAO.getSyncQueue(username);

    return {
      queuedCount: queueItems.length,
      ongoingJobs: false, // No ongoing jobs since files are processed directly
      queueItems,
    };
  }
}
